% This file is part of the ML4A project.
% Copyright 2017, 2018 the author.

% style notes:
% ------------
% - Am I consistent about the plurality of `data'?
% - Don't get cute with the footnotes.
% - Only use \section{}, \subsection{}, \paragraph{}, and description list. Nothing else for structure.

\documentclass[12pt, twoside, letterpaper]{article}
\usepackage{xcolor} % for todo shih
\usepackage{amsopn} % for DeclareMathOperator

% text macros
\newcommand{\thistitle}{Machine learning for astronomers}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\sectionnames}{Sections}
\newcommand{\equationname}{equation}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\vs}{\foreign{vs}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\PCA}{\acronym{PCA}}
\newcommand{\SVD}{\acronym{SVD}}
\newcommand{\KDE}{\acronym{KDE}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\SDSS}{\project{\acronym{SDSS}}}
\newcommand{\Kepler}{\project{Kepler}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}  % gotta have \usepackage{xcolor} in main doc or this won't work

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}
\DeclareMathOperator*{\argmin}{argmin}

% typesetting issues
\makeatletter
\renewcommand\Large{\@setfontsize\large\@xivpt{18}}
\makeatother
\renewcommand{\large}{\normalsize}  % punking article.cls
\linespread{1.08333} % 10/13 spacing
\setlength{\topmargin}{-0.25in}
\setlength{\textheight}{9.25in}
\setlength{\headheight}{2ex}
\setlength{\headsep}{4ex}
\addtolength{\textheight}{-\headheight}
\addtolength{\textheight}{-\headsep}
\setlength{\parindent}{1.10\baselineskip}
\pagestyle{headings}\markboth{\textsc{david w. hogg}}{\textsc{\MakeLowercase{\thistitle}}}
\renewcommand{\sectionmark}[1]{}    % punking article.cls -- headings
\renewcommand{\subsectionmark}[1]{} % punking article.cls -- headings
\renewcommand{\thefootnote}{{\bfseries{\arabic{footnote}}}}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing\thispagestyle{empty} % trust in Hogg

\section*{\thistitle%
\footnote{This \documentname\ is copyright 2017, 2018 the author. Feel free to copy and
distribute it, provided that you make no changes to it whatsoever.}}

\noindent
\textbf{David W. Hogg%
\footnote{For many of the ideas in this \documentname, I am indebted to
  Jo Bovy (Toronto),
  Rob Fergus (NYU),
  Dan Foreman-Mackey (Flatiron),
  Jennifer Hill (NYU),
  Iain Murray (Edinburgh),
  Sam Roweis (deceased),
  and
  Bernhard Sch\"olkopf (MPI-IS).
It is also a pleasure to thank
  Alessandro Gentilini for some help with the manuscript.}}\\
\textsl{\footnotesize
  Center for Cosmology and Particle Physics, Department of Physics, New York University \\
  Center for Data Science, New York University \\
  Max-Planck-Institut f\"ur Astronomie, Heidelberg \\
  Flatiron Institute, a division of the Simons Foundation}

\paragraph{Abstract:}
Machine learning---which is hard to define uncontroversially
but which involves expertise-eschewing, data-driven methods for classification,
regression, dimensionality reduction, density estimation, and clustering,
with methods that have great flexibility or even non-parametric form---has transformed all
of the sciences and commerce.
Naturally, it is appearing in many contexts in astronomy and astrophysics and
in such numbers that any kind of review or analysis would now be a fool's errand.
Here I give a very personal discussion of machine-learning methods and lay out the costs and
benefits of their adoption.
And I provide some unsolicited advice!
I highlight the five simplest and most elegant methods I know: support vector
machines, linear regression, principal components analyis, Gaussian mixture models
with expectation maximization, and k-means.
I then discuss their limitations, generalizations, and alternatives, with a focus
on some key qualities:
For most astronomical contexts, we want methods that are interpretable (at least
partially), generalizable (in at least some respects), probabilistic (in that
there is something akin to a likelihood function involved, such that the method can
be inserted into some bigger inference with informed causal structure).
Very few high-performance or computationally tractable methods have any of these
properties, and therefore (in my view),
\emph{most machine-learning methods are not advisable for most astronomical
applications!}
Of course there are exceptions, which I discuss.
Additionally, I spend some time on the magic of kernels and non-parametrics, and in particular
I highlight the Gaussian process, which shows great promise for astronomy.
One of the primary goals of this \documentname\ is to attempt to set down in writing
some of the folk or tacit knowledge that is available in the community, but
(nearly) invisible in the literature.

\clearpage\section{Introduction}

\subsection{Why machine learning in astronomy?}

Astronomy---thanks to enormous support from public and private funding
agencies---is awash in beautiful and informative data, about planets, stars,
black holes, accretion flows,
the interstellar medium, the Milky Way, the local Universe, and the entire
Hubble Volume, including the surface of last scattering.
For all of these observed phenomena, we have physical models, executed
with large computational simulations, of varying degrees of maturity,
that explain the observations.
But of course the data are so rich and so informative and so featured
that no physical model (yet) does justice to all of the details of these
data.
That is, there is far more to see and learn from the data than we can
capture with our (necessarily simplified and limited) physical models.
In some cases, there isn't even a working physical model at all.

It is tempting, therefore, to attempt to transform some of the important
scientific questions in astronomy from questions about how physical models are
constrained by data into a more data-driven form.
That is, when the data are far more featured than any model, it is interesting
to ask whether the data can somehow \emph{become} the model.
And indeed, sometimes questions \emph{can} be so transformed.
For example, when we are looking for tiny, periodic exoplanet transit
signals in the noisy light curve of a variable star, we don't necessarily
need an accurate physical model of the stellar variability:
We can use the empirical properties of the variability learned from
other, similar stars.
When we want to distinguish stars from quasars in multi-band imaging, we
don't necessarily need accurate physical models for stars and quasars,
we can use known stars and quasars to teach us the differences.
In cases like these, the data \emph{is} the model.\footnote{\todo{Something about this phrase.}}

Without digressing into a long discussion of precision and accuracy, I
think it is worth noting here that data-driven methods can achieve
unparalleled accuracy for some tasks:
After all, the observed data provide---or can provide---a very
\emph{accurate} model or picture of astrophysical phenomena \emph{in
  the space of the data}.
That is, if the goal is a model that reproduces the data (as it is in
the planet-finding and classification tasks I just mentioned), a
data-driven model is almost guaranteed to beat any physics-based
model.\footnote{\todo{Argument about modeling the residuals, which is
    more-or-less a proof.}}

And there are other equally important tasks for which data-driven
models can achieve unparalleled \emph{precision}:
If a model is given the flexibility to capture informative features
in a data set that are not captured by other, cruder models, then that
model can use those features for inference.
That is, a sufficiently flexible data-driven model can discover information
sources in the data that more inflexible models might not capture.
This has been true, for example, in our data-driven models of stars\footnote{\todo{cite Ness etc}},
where small issues with atomic, molecular, and plasma physics prevent
physical models of stellar photospheres from achieving the full precision
on (say) element abundances available in the data, but a data-driven
model does better (at least in terms of precision).

Here we are going to look at the space of data-driven models that are
referred to as ``machine learning''.
These are generally and currently the most flexible forms of data-driven
models, and they show great promise for some tasks in astronomy.

If I try to intuit the origins of the name ``machine learning'',
I would guess it relates back to a branch of artificial intelligence,
in which it became interesting to know whether a machine (that,
presumably, does not have the experience or assumptions of a human)
can perform scientific tasks that involve generalizing or learning from
data. \todo{Put a footnote here with a corrective to this.}
Indeed, some of my best friends still use the term
this way, counting any computational inference (any use of a machine
to learn parameters or properties of a data set) as machine learning.
Here I am going to use the term more narrowly, to refer to the computational
inferences that are very scientifically agnostic, that make use of extremely
flexible (highly parameterized or even non-parametric) models, and
that are defined with no reference to the particular data or domain in
which they work.
These include many things you have heard (or maybe even used),
including principal components analysis, convolutional neural
networks, random forests, \todo{[t-SNE here]}, auto-encoders,
generative adversarial networks, support vector machines,
and Gaussian processes,
just to name a few\footnote{This short list (given
  that it includes generative adversarial networks but does not
  include whatever's next) will stand as a very precise time-stamp for
  when this \documentname\ was written.}
that have appeared in the astronomical literature.

So where is machine learning being used in astronomy and why?  Here is
an incomplete list, with just a few instances cited.\footnote{This
  list is not anything like a review of the literature; please don't
  treat it as such!}
\begin{description}
\item[classification]
...Classification problems that humans can solve but expert systems
can't. Galaxy Zoo post-process; RF variable stars.
\item[structured nuisances]
...Nuisances we don't care about, like galaxy SEDs; all we want is
photo-zs. Also the star--quasar and galaxy-star classification
problems. And The Cannon.
\item[visualization and discovery]
...Visualization and discovery in complex data sets, like t-SNE in RAVE
or k-means in APOGEE. Or RF in SDSS.
\item[operational monitoring]
...Operational situations: Are telescopes operating normally? I don't know
of cases here.
\item[speeding computation]
...Speeding computation; emulators.
\end{description}

\subsection{Who are you?}

...I am assuming that anyone reading this \documentname\ has a
significant fraction of the following properties:

...Make sure to cite Bishop as background reading.

...Warning: This \documentname\ will be extremely non-linear, because
the connections across different subjects are myriad. If you know what
you are doing, and all you are looking for is my advice, you
want to skip to \sectionname~\ref{sec:advice}, where I gather, repeat,
and summarize all the polemical statements I make throughout the text.
If you just want to skim the relevant
algorithm names to get some ideas for some application you have in
mind, you can just read (or skim) \todo{what?}. If you just want to arm
yourself with cynical, critical material for fending off machine-learning
proponents, you will want to concentrate on \todo{which?}.

\subsection{Basic taxonomy of machine-learning tasks}

There is a hierarchy of machine-learning tasks, with boundaries that
are a bit fuzzy.
At the top of this hierarchy is the dichotomy between \emph{supervised}
and \emph{unsupervised} methods.
In supervised methods, the idea is that there is a \emph{training set}
of data, for which there is some kind of data (in astronomy this would
be photometry or imaging or spectroscopy or light curves or something like those),
and there are also some kind of \emph{labels}.
These might be binary labels (star \vs\ quasar, for example) or
categorical labels (elliptical, spiral, lenticular, irregular, for
example) or continuous labels (temperature and surface gravity and
iron abundance, for example).
The machine-learning task is to learn---from the training set---the
relationship between the data and the labels, and then apply that
relationship to infer new labels for previously unlabeled data.

The primary supervised learning tasks are \emph{classification} (when
the labels are binary or categorical) and \emph{regression} (when the
labels are continuous).
As in many other mathematical contexts (optimization, root finding,
and so on), these tasks appear superficially similar, but the mathematical
techniques look very different, because cardinality.
We are going to talk about these tasks below, and even spell out for
each of them a very simple, very powerful algorithm.
Astronomers even with no machine-learning background are very used
to performing tasks that look like or are regressions:
These are basically equivalent to fitting functional relationships to
observed data.

In the unsupervised methods, the idea is that there are no labels,
there are just the data themselves.
The machine-learning method or algorithm is asked to find
structure in the data and describe or reveal or capitalize on that
structure somehow.
For example, think about typical images: They might have millions
of pixels each and each pixel might have 24 bits of information in it.
In principle, therefore, images live in a space that has a cardinality
(in base 10) that requires tens of millions of decimal places!
And yet, if you generate artificial data by just randomly choosing
integers of that size and visualizing them as images, almost none of
the numbers you will ever generate will look even the slightest bit
like any real image that would be taken by a camera or a telescope.
That is, real images live in a tiny, tiny subspace of all possible
images.
That is, the subspace of real images is highly informative.
That's just an overly specific example, but the point is that
unsupervised methods are designed to find and describe these kinds of subspaces.

Unsupervised methods subdivide (again, only fuzzily) into various
kinds of tasks.
In \emph{clustering}, the goal is to categorize the data into categories
of similar objects.
In \emph{dimensionality reduction}, the goal is to project or
transform the (usually high dimensional) data into a lower dimensional
space that is smaller but somehow captures or preserves most or all or
the most important structure.
In \emph{density estimation}, the goal is to describe the distribution
of data within the data space in a volumetric or measure sense.

In some sense, clustering and dimensionality reduction are very closely
related, with one being categorical and the other continuous.
They both try to replace the high dimensional data with a smaller number of
pieces of information (discrete classes or continuous features).
These are closely related in the
same sense that classification and regression are closely related.
Density estimation is generally far harder than either clustering
or dimensionality reduction, especially as the dimensionality of the
data get large, because it is hard to build models (or have training
data) that have support in a large fraction of even any relevant
subspace of any high dimensional space.

In the next sections I am going to describe in some detail one
canonical algorithm or method for each of these five tasks
(classification, regression, clustering, dimensionality reduction,
and density estimation).
I will choose these five algorithms on the basis of their beauty, their
simplicity, their power, and their pedagogical value.
I will follow the canonical example algorithm descriptions with discussions
of limitations, extensions, and far more powerful algorithms, along
with some barely justifiable advice.
These discussions are followed by sections that address statistical,
conceptual, scientific, and mathematical considerations that cut
across applications and algorithms.

\section{Classification}

...Supervised. In astro: Star--galaxy, high-redshift quasars, cosmic rays.

\subsection{Example: SVM}

...SVM

\subsection{Advanced methods}

...kSVM. Concept of non-parametrics, to be discussed below.

...logistic regression

...Rando Forest

...NNs, CNNs, RNNs, GANs, etc

...What we did for XDQSO

\subsection{Philosophy and advice}

...What is classification to a bayesian, and how does this relate?

...Noise models; data of different quality; missing data.

...Generalizability.

\section{Regression}

Regression is just the same as classification, but for the case
in which the class labels are continuous variables.
For example, with \project{The Cannon}, we used a regression
to determine the effective temperature, surface gravity, and
metallicity of stars with a data-driven model, where the data
were stellar spectra.\footnote{\todo{Cite}}
For another example, in photometric-redshift schemes, the idea
is to use purely photometric or imaging data on galaxies or quasars
to infer their redshifts,
which are continuous labels.\footnote{\todo{Cite lots}}
One major use of regression is to remove nuisances from complex
data:
In (for example) \project{Kepler} light curves, the systematics
introduced by the spacecraft pointing and thermal state put
big signals into the data in which we are looking for exoplanet
transits.
Common practice is to use a regression to predict and remove
(or better, simultaneosly model) these systematics when searching
for exoplanets.\footnote{\todo{Cite lots}}

\subsection{Example: Linear Regression}

The absolutely \emph{simplest and most underappreciated} machine-learning
method on the books is \emph{linear regression}.
With good engineering, and a few tricks (which we discuss below), linear
regression can be extremely powerful, and exceedingly fast.\footnote{%
  \todo{Something about \project{The Cannon} here.}}
In linear regression,
we are going to permit an arbitrary, large linear combination of data to predict
the labels.

As with classification, we have an $N\times D$ rectangular block of data $Y$,
and a $N$-vector (or $N\times 1$ rectangular block) of labels $X$.
But this time the labels will be continuous rather than discrete.
...Linear regression!

...Going up in powers or functions of data
...preview kernel trick

...Regularization L2 and L1.

...There can be more than one label! Consider $N\times L$ labels.
...Simplest case: Do each one independently!

...Advantages and disadvantages of this method.

\subsection{Advanced methods}

...Rando Forest, CNNs, and all the stuff from last section.

...Is there a regression form of SVM and kSVM?

...Gaussian Processes (and we will elaborate more later).
Likelihood function re-appears!
Connection to kernel trick.

\subsection{Philosophy and advice}

...Importance of noise models.

...Knowledge about smoothness, etc.

\section{Dimensionality Reduction}

Dimensionality reduction is the first of the three unsupervised
method categories to be discussed.
It has been used to great effect in astronomy.
For example, galaxy spectra vary over a huge domain, but that
domain is (believed to be) much lower in dimensionality than the size
of the spectral data space (which can be thousands or even tens of
thousands of pixels in size).
And indeed, the dimensionality reductions that have been performed on
galaxy spectra find that a few principal vectors can explain much of
the diversity seen in galaxy spectra.\footnote{\todo{cite Press and
    others.}}
And, furthermore, the first few principal components look very much
like interpretable aspects of galaxies: Old stellar populations, young
stars, and line emission.
Projects like this kicked off dimensionality reduction in astronomy,
and gave strong endorsement to the very simple and beautiful method
known as principal components analysis, which I am going to describe
here, and then criticize.

Most uses of dimensionality reduction in astronomy have been for 
what a machine learner might call ``feature engineering'': You want
to measure galaxy redshifts with a regression, but you want to make the regression
lower in dimensionality while retaining most of the information
in the data.\footnote{cite \SDSS\ pipelines here.}
Or the same but for speckly residuals in a coronograph.\footnote{shameless plug here.}
We will say a more about feature engineering below.

\subsection{Example: PCA}

Principal components analysis (\PCA) is the simplest of all dimensionality
reductions: It is linear, and it is convex.\footnote{\todo{Define convex.}}
Once again, we have data $Y$, which is a rectangular $N\times D$ array of
measurements ($N$ instances of $D$-vectors, we can think of it, or the transpose
of that).
But we are unsupervised now, so we don't have any labels for these data, only
the data themselves.
There are several ways to formulate \PCA, but the most astronomy-friendly
is probably the following:
We want to find an approximate factorization of the data matrix
\begin{equation}
Y\T = W \cdot X + S
\quad ,
\end{equation}
where $W$ is a $D\times K$ rectangular block of $K$ eigenvectors,
$X$ is a $K\times N$ block of coefficients,
and $S$ is a $D\times N$ array containing the residuals away from that
(necessarily approximate) factorization.
But we want to find the factorization that minimizes 
the sum of squares of the elements of $S$.
In optimization language, \PCA\ can be written as
\begin{equation}\label{eq:pcaopt}
\argmin_{W, X} ||Y\T - W \cdot X||_2^2
\quad ,
\end{equation}
where the $||S||_2^2$ notation represents the L2-norm of $S$, or the sum of squares
of the elements of $S$.\footnote{\todo{Brief digression on norms and notation?}}
It turns out that this optimization is solved exactly (up to rotation and scaling
degeneracies) by taking the singular value decomposition (\SVD) of the matrix $Y$
and using the top-$K$ eigenvectors as $W$.
\emph{That is magical and valuable.}
The $X$ array also comes out of the \SVD, or you can get it by projecting the
data $Y$ onto
the eigenvectors in $W$, so long as the eigenvectors are orthonormal (which they
are, when produced by the \SVD).

In practice usually we subtract a mean from $Y$ before performing \PCA.
That is, find the mean $D$-vector by averaging $Y$ over the $N$ axis,
and then subtracting that $D$-vector from each of the $N$ $D$-vectors
in $Y$.
This isn't necessary, but it is sensible, because \PCA\ is a description
of the \emph{variance} of the data $Y$.

Along those lines:
Another description of \PCA\ is that it is an attempt to find the directions of
maximum (empirical) variance.
That is, if you construct the $D\times D$ empirical variance tensor $V$ by
\begin{equation}
V = \frac{1}{N}\,Y\T \cdot Y
\end{equation}
(or again the same after subtracting the mean),
then the top-$K$ \PCA\ eigenvectors in $W$ will be the $K$ directions (or $K$-dimensional
linear subspace) that contains the largest fraction of the empirical variance in the data.
That is another description of the method; it is equivalent
because the objective that is being minimized in \equationname~(\ref{eq:pcaopt})
is proportional to the variance of the residual.
In this formalism, the $D$-vectors in $W$ are the $K$ largest-eigenvalue
eigenvectors of the empirical variance tensor, found by diagonalization of $V$.
In general it is faster and more numerically stable to take the \SVD\ of $Y$
than compute the diagonalization of $V$, but these descriptions are mathematically
equivalent.

...Going further...rescaling objects b/c brightness; rescaling data dimensions b/c units.
...Relation to feature engineering.

...Mention the dual-space version -- complete symmetry between the $N$-vectors and the $D$-vectors.

\PCA\ is a remarkable and versatile algorithm.
It is fast and convex.
When run on very good data (and more on what I mean by this below), it returns
sensible results, often even (apparently) interpretable results.
It has enabled scientific results in a range of astrophysical contexts.

However, \PCA\ also has enormous disadvantages.
The first disadvantage is that it is an algorithm to describe the full,
empirical variance in the data.
That is, it makes no distinctions between variance that comes from (say)
instrument noise, or from (say) variation in galaxy star-formation histories,
or from (say) cosmic-ray hits.
It mashes all sources of variance together and treats them together.
This makes the components (technically) uninterpretable (and relates to the
fact that the components only become apparently interpretable as the data become good---high
in signal-to-noise and free of outliers).
Relatedly, the second disadvantage of \PCA\ is that it's objective function
(the L2 norm of the residuals) treats every data point
as equally important.
There is no weighting or noise model at all, and no capacity for handling
missing data.
For this reason, a data set with outliers, missing data, or bad data can deliver
principal components that are highly corrupted.
A third disadvantage is that the algorithm is precisely linear; it only
considers linear projections of the data.
Often the structure is non-linear!
A fourth is that the algorithm implicitly adopts an isotropic distance metric
in the $D$-dimensional data space, 
in which all directions in the data space are equally important; often there
are some directions that we know are more important than others.
A fifth is that the algorithm implicitly treats all $N$ data points as equally
important; often we know that some are much better measured (or much more important)
than others.
A sixth---and like many of the above objections, this objection applies to
many dimensionality-reduction methods---is that the user must choose the integer
$K$.
There is no principled or even probabilistic method for choosing $K$ without
substantially extending or modifying the method.
Many of the above objections apply not just to \PCA\ but to many of the methods
described throughout this document.
The reader might sense some repetition here and in the \sectionnames to follow.

One way to phrase my objection to \PCA\ is that it is an \emph{algorithm}
and not a \emph{model}.
It does not have stated (or even stateable) assumptions.
It is just an arithmetic procedure applied to data.
That said, the procedure has great properties, is easy to use, and has
delivered value for astronomy.

The output of \PCA\ can be used in a few ways:
The $W$ array of eigenvectors can be used now as a new basis that represents a
data-driven subspace of the original data.
This is how we use it (say) when we want a data-driven set of vectors for fitting
\Kepler\ light curves of stars.\footnote{Cite DFM, Luger.}
But also the $X$ array of coefficients can be used as a dimensionality-reduced,
rotated data space.
Often any clustering or nonlinear structure in the original data is clearer in
the coefficient space.
\todo{Astronomical example: When is this the use case?}
But there is another way to use the output:
The multiplies $W\cdot X$ can be used as replacements for the data!
If the data are pretty good, and the integer $K$ is chosen well, then these
low-rank approximations to the data will be (or at least look like) de-noised
versions of the data.
\todo{Astronomical example of when this is used.}

\subsection{Advanced methods}

...Just like kSVM: kPCA. Why don't astronomers use this? The Dual problem problem.

...Use kPCA to feed into Manifold-learning and t-SNE.
Relationships to visualization and feature engineering (more below).

To fix the objection that \PCA\ is not probabilistic, or doesn't have well
defined assumptions---it isn't a model, really---there have been several
extensions of \PCA:
...PPCA, FA

...latent-variable models like GPLVM

...Robust PCA -- both the good and bad versions of that.

...auto-encoders: Probably useful!

...GANs

\subsection{Philosophy and advice}

\section{Clustering}

...Unsupervised. In astro: SNe types, spectral types.

\subsection{Example: k-means}

...k-means!

\subsection{Advanced methods}

...[Here I am weakest]

\subsection{Philosophy and advice}

...Using classifiers or regressions to find outliers. (The opposite of clustering.)

\section{Density estimation}

The most difficult of all unsupervised tasks is density estimation:
Given a set of points drawn from a probability density function (pdf),
infer the density everywhere.
This task is technically impossible in high dimensions, because it is
impossible to have enough data to critically sample a pdf in a (say)
thousand-dimensional space.\footnote{Estimates vary, but the number of points
  required seems to scale \emph{faster} than the exponential of the dimension $D$,
  something more like $D$ factorial. For $D=1000$, that's a big number.}
That said, in the next \sectionname, I will give examples of some methods
that have success at large $D$.

In astrophysics, density estimation comes up in a few places:
One is if you are performing a Bayesian inference, maybe with a cosmological simulation
as part of your model, and you can only afford to take a few samples from your posterior
pdf (which lives in some parameter space).
If you want to have some idea of the posterior pdf everywhere, you
need a density estimate.\footnote{\todo{Cite Marshall here, and also
    Tollerud?}}
Another place for density estimation is in galaxy redshift surveys:
In order to measure the clustering of the large-scale structure, you need to
know the radial selection function of the survey, but your best data for inferring
it are the observed redshifts in the data sample itself.\footnote{\todo{cite}}
Another is outlier identification: Is a particular stellar spectrum (say) an
outlier away from the distribution of stellar spectra?
You can't answer that if you don't know where stellar spectra lie in spectrum space.

The simplest density estimate is the kernel density estimate (\KDE).
A \KDE\ is a smoothing of the data themselves.
The density estimate is the...

\subsection{Example: GMMs with EM}

The most beautiful model 
...GMM with E-M algo!

...Relationship to clustering and various confusions around that.

\subsection{Advanced methods}

...XD, and why we care. And how did this become a classification algorithm? Connections there.

...RNADE

...GANs: In what sense do these estimate density?

\subsection{Philosphy and advice}

\section{Cross-cutting themes}

\subsection{Implementation, packages, hardware}

\subsection{Rectangular data}

...format

...but also issues with missing data (to return below)

...kinds of data that can't be made rectangular (trivially). Specifically mention variable-star light curves.

...importance of feature engineering if you must go rectangular.

...importance of distance metrics if you can't!

\subsection{Distance metrics}

...Most ML methods are nearest-neighbor methods, in some sense!

...Why chi-squared can't be your distance, in general (though sometimes!).

...Strict requirements on distance metrics.

...Loose advice for distance metrics.

\subsection{The kernel trick}

...Two ways to look at the kernel trick. One is as a change to the distance relation.

...The other is as a lifting of the data to a larger space.

...Strict requirements on the kernel.

...General way to make any kind of kernel! Mention / thank O'Neil here.

...Loose advice for kernels.

\subsection{Concept of non-parametrics}

...Infinite dimensional! More parameters than data! What's up?

...Call back to the kernel trick here.

\subsection{Feature engineering}

\subsection{Statistical principles}

...train, validate, and test.

...concepts of statistical conservatism

...stationarity requirements

\subsection{Active learning}

\section{Interpretability, probabilism and causal structure}

\subsection{Interpretability}

\subsection{Generalizability}

\subsection{Probabilistic methods}

\subsection{Gaussian processes}

\section{Unsolicited advice}\label{sec:advice}

\todo{Make here a full description-style list of all the
  advice I have given through the document.}

...only GPs are fully probabilistic and non-parametric

...model nuisances

...model or locate outliers

...operational contexts

...emulators

\end{document}
